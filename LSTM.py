import numpy as np
import math

def sigmoid(x):
    return 1 / (1 + math.exp(-x))

class LSTMCell: 

    # Size is the dimensionality of the input vector
    def __init__(self, size, numCells):
        self.size = size
        self.numCells = numCells

        self.W_f = np.zeros((numCells, size)) # Forget gate matrix (for input)
        self.W_i = np.zeros((numCells, size)) # Input gate matrix (for input)
        self.W_c = np.zeros((numCells, size)) # Candidate value matrix (for input)
        self.W_o = np.zeros((numCells, size)) # Output gate matrix (for input)

        self.U_f = np.zeros((numCells, numCells)) # Forget gate matrix (for prev output)
        self.U_i = np.zeros((numCells, numCells)) # Input gate matrix (for prev output)
        self.U_c = np.zeros((numCells, numCells)) # Candidate value matrix (for prev output)
        self.U_o = np.zeros((numCells, numCells)) # Output gate matrix (for prev output)

        self.h = np.zeros(numCells) # output at current time
        self.C = np.zeros(numCells) # state at current time

    # x is the input vector (including bias term), returns output h
    def forwardStep(self, x):
        W_1 = np.concatenate((self.W_c, self.U_c), axis=1)
        W_2 = np.concatenate((self.W_i, self.U_i), axis=1)
        W_3 = np.concatenate((self.W_f, self.U_f), axis=1)
        W_4 = np.concateneate((self.W_o, self.U_o), axis=1)

        W = np.concatenate((W_1, W_2, W_3, W_4), axis=0)
        
        I = np.concatenate((x, self.h))

        z = np.dot(W, I)

        # Compute the candidate value vector
        self.C-bar = np.tanh(z[0:self.numCells])

        # Compute input gate vector
        self.i = sigmoid(z[self.numCells:self.numCells * 2])

        # Compute forget gate vector
        self.f = sigmoid(z[self.numCells * 2:self.numCells * 3])

        # Compute the output gate vector
        self.o = sigmoid(z[self.numCells * 3:])

        # Compute the new state vector as the elements of the old state allowed
        # through by the forget gate, plus the candidate values allowed through
        # by the input gate
        self.C = np.dot(self.f, self.C) + np.dot(self.i, self.C-bar)

        # Compute the new output
        self.h = np.dot(self.o, np.tanh(self.C))

        return self.h

    def forwardPass(self, x):
        outputs = []
        for i in range(len(x)):
            outputs.append(self.forwardStep(x[i]))
        return outputs

    def backwardStep(self, dE_dh_t, dE_dc_tplus1):
        # Need to compute things for this step... should probably just save when
        # first computed
        # generated by forwardStep
        
        dE_do_t = np.dot(dE_dh_t, np.tanh(self.C))

        dE_dc_t = dE_dc_tplus1 + np.dot(np.dot(dE_dh_t, self.o), (np.ones(self.numCells) - np.square(np.tanh(self.C))))



        dE_di_t = np.dot(dE_dc_t, self.C-bar)

        dE_dcbar_t = np.dot(dE_ct_t, self.i)

        dE_df_t = np.dot(dE_ct_t, self.C_tminus1)

        dE_dc_tminus1 = np.dot(dE_dc_t, self.f)


        dE_dzcbar_t = np.dot(dE_dcbar_t, (np.ones(self.numCells) - np.square(np.tanh(z[0:self.numCells]))))
        dE_dzi_t = np.dot(np.dot(dE_di_t, self.i), (np.ones(self.numCells) - self.i))
        dE_dzf_t = np.dot(np.dot(dE_df_t, self.f), (np.ones(self.numCells) - self.f))
        dE_dzo_t = np.dot(np.dot(dE_do_t, self.o), (np.ones(self.numCells) - self.o))

        dE_dz_t = np.concatenate(dE_dzcbar_t, dE_dzi_t, dE_dzf_t, dE_dzo_t)

        dE_dI_t = np.dot(np.transpose(W), dE_dz_t)



        dE_dh_tminus1 = dE_dI_t[self.size:]

        dE_dW_t = np.dot(dE_dz_t, np.transpose(dE_dI_t)) # this one is confusing cos it says X_t instead of I_t, but there is no matrix or vector X

        return (dE_dW_t, dE_dh_tminus1, dE_dc_t)



    # Back propagation through time, returns the error and the gradient for this sequence
    # (should I give this the sequence x1,x2,... so that this method is tied
    # to the sequence?)
    def BPTT(self, y, numTimePeriods):
        E = 0.5 * np.square(self.h - y) # This is the error vector for this sequence
        dE_dh_t = self.h - y # This is the error gradient for this sequence
        dE_dc_t = 0

        dE_dW = 0 
        for i in range(numTimePeriods):
            result = backwardStep(dE_dh_t, dE_dc_t)
            dE_dW = dE_dW + result[0] # dE_dW_t
            dE_dh_t = result[1]
            dE_dc_t = result[2]

        return (E, dE_dW)

class LSTMNetwork:

    # Structure is a vector specifing the structure of the network - each
    # element represents the number of nodes in that layer
    def __init__(self, structure):
        self.layers = [[x for x in structure]] # this doesnt make sense
        for 
