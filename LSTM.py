import numpy as np
import math

def sigmoid(x):
    return 1 / (1 + math.exp(-x))

class LSTMCell: 

    # Size is the dimensionality of the input vector
    def __init__(self, size, numCells):
        self.size = size
        self.numCells = numCells

        self.W_f = np.zeros((numCells, size)) # Forget gate matrix (for input)
        self.W_i = np.zeros((numCells, size)) # Input gate matrix (for input)
        self.W_c = np.zeros((numCells, size)) # Candidate value matrix (for input)
        self.W_o = np.zeros((numCells, size)) # Output gate matrix (for input)

        self.U_f = np.zeros((numCells, numCells)) # Forget gate matrix (for prev output)
        self.U_i = np.zeros((numCells, numCells)) # Input gate matrix (for prev output)
        self.U_c = np.zeros((numCells, numCells)) # Candidate value matrix (for prev output)
        self.U_o = np.zeros((numCells, numCells)) # Output gate matrix (for prev output)

        self.h = np.zeros(numCells) # output at current time
        self.C = np.zeros(numCells) # state at current time

    # x is the input vector (including bias term), returns output h
    def forwardStep(self, x):
        W_1 = np.concatenate((self.W_c, self.U_c), axis=1)
        W_2 = np.concatenate((self.W_i, self.U_i), axis=1)
        W_3 = np.concatenate((self.W_f, self.U_f), axis=1)
        W_4 = np.concateneate((self.W_o, self.U_o), axis=1)

        W_5 = np.concatenate((z_1, z_2), axis=0)
        W_6 = np.concatenate((z_3, z_4), axis=0)

        W = np.concatenate((z_5, z_6), axis=0)
        
        I = np.concatenate((x, self.h))

        z = np.dot(W, I)

        # Compute the candidate value vector
        self.C-bar = np.tanh(z[0:self.numCells])

        # Compute input gate vector
        self.i = sigmoid(z[self.numCells:self.numCells * 2])

        # Compute forget gate vector
        self.f = sigmoid(z[self.numCells * 2:self.numCells * 3])

        # Compute the output gate vector
        self.o = sigmoid(z[self.numCells * 3:])

        # Compute the new state vector as the elements of the old state allowed
        # through by the forget gate, plus the candidate values allowed through
        # by the input gate
        self.C = np.dot(self.f, self.C) + np.dot(self.i, self.C-bar)

        # Compute the new output
        self.h = np.dot(self.o, np.tanh(self.C))

        return self.h

    def forwardPass(self, x):
        outputs = []
        for i in range(len(x)):
            outputs.append(self.forwardStep(x[i]))
        return outputs

    def backwardStep(self, dE_dh_t, dE_dc_tplus1):
        # Need to compute things for this step... should probably just save when
        # first computed
        # generated by forwardStep
        
        dE_do_t = np.dot(dE_dh_t, np.tanh(self.C))

        dE_dc_t = dE_dc_tplus1 + np.dot(np.dot(dE_dh_t, self.o), (np.ones(self.numCells) - np.square(np.tanh(self.C))))



        dE_di_t = np.dot(dE_dc_t, self.C-bar)

        dE_dcbar_t = np.dot(dE_ct_t, self.i)

        dE_df_t = np.dot(dE_ct_t, self.C_tminus1)

        dE_dc_tminus1 = np.dot(dE_dc_t, self.f)



    # Back propagation through time
    def BPTT(self, y, numTimePeriods):
        dE_dh_t = self.h - y
        for i in range(numTimePeriods):
            backwardStep(dE_dh_t, dE_dc_t)
            # right now don't even have sprint football


class LSTMNetwork:

    # Structure is a vector specifing the structure of the network - each
    # element represents the number of nodes in that layer
    def __init__(self, structure):
        self.layers = [[x for x in structure]] # this doesnt make sense
        for 
